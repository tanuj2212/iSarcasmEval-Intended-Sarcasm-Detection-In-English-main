{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKaJz5j_ylj"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEfSbAA4QHas",
        "outputId": "e3b6edba-fc91-49b1-84e4-82a2e2d27194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "0NmMdkZO8R6q",
        "outputId": "164cb53a-5425-441e-f994-1ca205e713b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.46-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.2)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.8 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.46\n",
            "  Downloading botocore-1.23.46-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 94.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 92.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.46->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.46->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 89.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.46 botocore-1.23.46 jmespath-0.10.0 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "99ca7231-2a1b-4106-ed10-dc280b773b3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oR7zXtM2j_XV"
      },
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(\"sem18(train+test)and sem22(train with data aug).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this dataset consist of semeval training and test data + sem22 training data with data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "xfOe7C19kgTJ",
        "outputId": "dad8d92f-e6cb-4a59-eb0a-a06eb937d4e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-af193145-22be-4f12-89de-3e367a107180\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_headlines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10231</th>\n",
              "      <td>775</td>\n",
              "      <td>if you drag yesterday into today , your tomorr...</td>\n",
              "      <td>0</td>\n",
              "      <td>if you drag yesterday into today , your tomorr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10232</th>\n",
              "      <td>776</td>\n",
              "      <td>congrats to my fav &lt;user&gt; and her team and my ...</td>\n",
              "      <td>0</td>\n",
              "      <td>congrats to my fav and her team and my birthpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10233</th>\n",
              "      <td>777</td>\n",
              "      <td>&lt;user&gt; jessica sheds tears at her fan signing ...</td>\n",
              "      <td>0</td>\n",
              "      <td>jessica sheds tears at her fan signing event m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10234</th>\n",
              "      <td>778</td>\n",
              "      <td>al jazeera is pro anti &lt;hashtag&gt; gamer gate &lt;/...</td>\n",
              "      <td>1</td>\n",
              "      <td>al jazeera is pro anti gamer gate because femi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10235</th>\n",
              "      <td>779</td>\n",
              "      <td>&lt;allcaps&gt; all &lt;/allcaps&gt; 👌 there good and bad ...</td>\n",
              "      <td>0</td>\n",
              "      <td>all 👌 there good and bad in every occupation a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10236 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af193145-22be-4f12-89de-3e367a107180')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af193145-22be-4f12-89de-3e367a107180 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af193145-22be-4f12-89de-3e367a107180');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0  ...                                    clean_headlines\n",
              "0               0  ...  the biggest only problem thing i got from coll...\n",
              "1               1  ...  the absolutely only thing i got fired from the...\n",
              "2               2  ...  perhaps the second only nice thing i got out f...\n",
              "3               3  ...  i love it when college professors randomly dra...\n",
              "4               4  ...  i really love it funny when professors constan...\n",
              "...           ...  ...                                                ...\n",
              "10231         775  ...  if you drag yesterday into today , your tomorr...\n",
              "10232         776  ...  congrats to my fav and her team and my birthpl...\n",
              "10233         777  ...  jessica sheds tears at her fan signing event m...\n",
              "10234         778  ...  al jazeera is pro anti gamer gate because femi...\n",
              "10235         779  ...  all 👌 there good and bad in every occupation a...\n",
              "\n",
              "[10236 rows x 4 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hAdVCC5Ykk4d"
      },
      "outputs": [],
      "source": [
        "df2 = df2[[\"tweet\",\"sarcastic\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "boWnOPlIk6vx"
      },
      "outputs": [],
      "source": [
        "df = df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AQfTaYDo42zu",
        "outputId": "cc82908d-17e0-4509-bec6-defe29953e28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0f9731f7-c1c0-422a-b6b7-1000751eea24\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8667</th>\n",
              "      <td>&lt;url&gt; because , progress . &lt;repeated&gt; &lt;hashtag...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3937</th>\n",
              "      <td>today im grateful for: \\r\\n•old friends\\r\\n   ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6178</th>\n",
              "      <td>&lt;user&gt; lol of course it ' s the best ! &lt;repeated&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9850</th>\n",
              "      <td>merry christmas to all my friends and family ....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>. @ taylorswift13 is 100 % excited going out t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6195</th>\n",
              "      <td>i have a &lt;allcaps&gt; very &lt;/allcaps&gt; limited num...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>@ xnemoooo @ mcdonaldsuk @ u tasshx say i swea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2866</th>\n",
              "      <td>WHO LET THE LAUNDRY PILE UP 5 BASKETS. Goodness.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3014</th>\n",
              "      <td>why is the weather having a mid life crisis</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>today i was only in taylor swift records ’ ’ s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f9731f7-c1c0-422a-b6b7-1000751eea24')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f9731f7-c1c0-422a-b6b7-1000751eea24 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f9731f7-c1c0-422a-b6b7-1000751eea24');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  tweet  sarcastic\n",
              "8667  <url> because , progress . <repeated> <hashtag...          1\n",
              "3937  today im grateful for: \\r\\n•old friends\\r\\n   ...          0\n",
              "6178  <user> lol of course it ' s the best ! <repeated>          1\n",
              "9850  merry christmas to all my friends and family ....          0\n",
              "680   . @ taylorswift13 is 100 % excited going out t...          1\n",
              "6195  i have a <allcaps> very </allcaps> limited num...          0\n",
              "813   @ xnemoooo @ mcdonaldsuk @ u tasshx say i swea...          1\n",
              "2866   WHO LET THE LAUNDRY PILE UP 5 BASKETS. Goodness.          1\n",
              "3014        why is the weather having a mid life crisis          1\n",
              "57    today i was only in taylor swift records ’ ’ s...          1"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3DjEe9eeCm_",
        "outputId": "b04457be-f984-4aba-9a17-c1a3fc678dfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    5446\n",
              "0    4790\n",
              "Name: sarcastic, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"sarcastic\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1107
        },
        "id": "9KBjQpMdhvIK",
        "outputId": "efc9e6ca-e5bd-470e-e84b-16faa786a8e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-292a8b08-ee39-4343-b3e6-c18cea9c71dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>thing got college caffeine addiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>love professor draw big question mark next ans...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>remember hundred email company covid started g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>today pop-pop told “forced” go college 🙃 okay ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>too, also reported cancun cruz worrying heartb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13551</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "      <td>8-9ft man found ancient indian burial mound', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13552</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>\"second scottish independence referendum 'on t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13553</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "      <td>pinoy cyborg james simmons', 'mag-ingat sa rid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13554</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>the logic flawless!', \"no isn't, one 747 gutte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13555</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "      <td>til one founding member greenpeace left organi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13556 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-292a8b08-ee39-4343-b3e6-c18cea9c71dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-292a8b08-ee39-4343-b3e6-c18cea9c71dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-292a8b08-ee39-4343-b3e6-c18cea9c71dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  ...                                        clean_tweet\n",
              "0      The only thing I got from college is a caffein...  ...               thing got college caffeine addiction\n",
              "1      I love it when professors draw a big question ...  ...  love professor draw big question mark next ans...\n",
              "2      Remember the hundred emails from companies whe...  ...  remember hundred email company covid started g...\n",
              "3      Today my pop-pop told me I was not “forced” to...  ...  today pop-pop told “forced” go college 🙃 okay ...\n",
              "4      @VolphanCarol @littlewhitty @mysticalmanatee I...  ...  too, also reported cancun cruz worrying heartb...\n",
              "...                                                  ...  ...                                                ...\n",
              "13551  ['8-9ft man found in ancient indian burial mou...  ...  8-9ft man found ancient indian burial mound', ...\n",
              "13552  [\"Second Scottish independence referendum 'on ...  ...  \"second scottish independence referendum 'on t...\n",
              "13553  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...  ...  pinoy cyborg james simmons', 'mag-ingat sa rid...\n",
              "13554  ['The logic here is flawless!', \"No it isn't, ...  ...  the logic flawless!', \"no isn't, one 747 gutte...\n",
              "13555  ['TIL One of the founding members of Greenpeac...  ...  til one founding member greenpeace left organi...\n",
              "\n",
              "[13556 rows x 3 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n",
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n",
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n",
            "Error: Runtime no longer has a reference to this dataframe, please re-run this cell and try again.\n"
          ]
        }
      ],
      "source": [
        "databig1 = pd.read_csv(\"sarcastic_dataset_13556.csv\")\n",
        "databig1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this dataset taken from reedit "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "O7_5LN1qqhu8",
        "outputId": "444eb35d-1e86-4bbd-d566-dbd1f7788bed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b7755ec-033a-4605-90ec-926f0a1c9e1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13551</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13552</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13553</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13554</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13555</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13556 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b7755ec-033a-4605-90ec-926f0a1c9e1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b7755ec-033a-4605-90ec-926f0a1c9e1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b7755ec-033a-4605-90ec-926f0a1c9e1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  sarcastic\n",
              "0      The only thing I got from college is a caffein...          1\n",
              "1      I love it when professors draw a big question ...          1\n",
              "2      Remember the hundred emails from companies whe...          1\n",
              "3      Today my pop-pop told me I was not “forced” to...          1\n",
              "4      @VolphanCarol @littlewhitty @mysticalmanatee I...          1\n",
              "...                                                  ...        ...\n",
              "13551  ['8-9ft man found in ancient indian burial mou...          0\n",
              "13552  [\"Second Scottish independence referendum 'on ...          0\n",
              "13553  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...          0\n",
              "13554  ['The logic here is flawless!', \"No it isn't, ...          0\n",
              "13555  ['TIL One of the founding members of Greenpeac...          0\n",
              "\n",
              "[13556 rows x 2 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "databig1 = databig1.drop([\"clean_tweet\"],axis=1)\n",
        "databig1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JLTejvXZiG2y"
      },
      "outputs": [],
      "source": [
        "databig= df.append(databig1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AxnSF-rNiNyS",
        "outputId": "2db9a91f-071a-46e4-e463-e921029d2520"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2f0df32b-a277-4ed2-acff-e05f71fedd0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13551</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13552</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13553</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13554</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13555</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23792 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f0df32b-a277-4ed2-acff-e05f71fedd0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f0df32b-a277-4ed2-acff-e05f71fedd0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f0df32b-a277-4ed2-acff-e05f71fedd0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  sarcastic\n",
              "0      the biggest only problem thing i got from coll...          1\n",
              "1      the absolutely only thing i got fired from the...          1\n",
              "2      perhaps the second only nice thing i got out f...          1\n",
              "3      i love it when college professors randomly dra...          1\n",
              "4      i really love it funny when professors constan...          1\n",
              "...                                                  ...        ...\n",
              "13551  ['8-9ft man found in ancient indian burial mou...          0\n",
              "13552  [\"Second Scottish independence referendum 'on ...          0\n",
              "13553  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...          0\n",
              "13554  ['The logic here is flawless!', \"No it isn't, ...          0\n",
              "13555  ['TIL One of the founding members of Greenpeac...          0\n",
              "\n",
              "[23792 rows x 2 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "databig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_6QdEIi0jboa"
      },
      "outputs": [],
      "source": [
        "df=databig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXi04DAoj8-T",
        "outputId": "bfbdadb2-a3c7-4325-8ff8-310b66df7c0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Day thirteen of three hundred and sixty five . Retired Pope Benedict warns Francis against relaxing priestly celibacy rules CLICK HERE ---> > > <URL> @USER #mmandmp_pro #retweet #rt', \"Day thirteen of three hundred and sixty five . Taal volcano : Lava spews as ' hazardous eruption ' feared CLICK HERE ---> > > <URL> @USER #mmandmp_pro #retweet #rt\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               9\n",
              "['#Who_am_I Born in Japan . After high school , I made my rock band debut . I then founded a fashion EC ‘ ZOZOTOWN ’ . TSE-listed and gained a market worth of US $ 15 billion . In Sept . 2019 , I sold it to Softbank Group and stepped down as CEO . My net worth is now US $ 2 billion ( Forbes ) . <URL>', \"I have a passion for collecting contemporary art , Japanese antiques , supercars , wine etc . Some people know me for purchasing J.M.Basquiat ’ s $ 110 million painting . I will be the first civilian to fly around the moon on @USER ' s Starship rocket , scheduled to launch in 2023 . <URL>\", 'In 2019 , I did a 100 million yen giveaway on Twitter ( 1M yen to 100 winners ) . I hold the record for the most retweeted tweet . In 2020 , I did a giveaway of up to 1 billion yen . This social experiment will study the effects of UBI . Will you be happier if you were given 1M yen ? <URL>'     6\n",
              "@AsdaServiceTeam imagine your delivery being 2 hours late, and imagine calling up your service team only for them to hang up at 10pm, coincidentally the same time the office closes. But it’s okay, my £3 delivery fee is being refunded though 👊🏻                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           5\n",
              "[\"#Contest2 Okay , now this one's gonna be interesting ! If you had 1 million rupees and had to use it all on Myntra , what would you shop for ? Tell us using #MyntraEORS #EORSGameZone 5 lucky winners that make us go WHOA get Myntra vouchers worth Rs . 1000 . <URL>\", \"Come on guys ! We've still not got our winners . 1 Million is a lot ... Put your thinking caps on ! Remember , the wittiest answer wins ... Don't forget to #MyntraEORS #IndiasBiggestFashionSale #EORSGameZone\"                                                                                                                                                                                                                                                                                                                                                                                                                                 5\n",
              "['They are rigging the election again against Bernie Sanders , just like last time , only even more obviously . They are bringing him out of so important Iowa in order that , as a Senator , he sit through the Impeachment Hoax Trial . Crazy Nancy thereby gives the strong edge to Sleepy ...', '... Joe Biden , and Bernie is shut out again . Very unfair , but that ’ s the way the Democrats play the game . Anyway , it ’ s a lot of fun to watch !'                                                                                                                                                                                                                                                                                                                                                                                                                                                                 4\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ..\n",
              "oh hello flu ! thank you for fooling me in thinking you were gone ! i missed you so much ! <hashtag> flu </hashtag> <hashtag> yaay </hashtag>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
              "words haven ’ t really written in months cuz... there are no male language teachers to impress anybody [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
              "stop thinking about just how no one is obsessed with telling me [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
              "['Wells Fargo exec who headed up phony accounts unit walks away with $125 Million USD', '...and 5300 of the employees under her got fired.'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
              "[\"For the first time in my life I'm not proud to be an American #NotMyPresident\", '@USER do something . Change something . Or ... tweet about your personal feelings after 100 millions people made a decision .', '@USER I did do something . I chose not to vote for either candidate'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      1\n",
              "Name: tweet, Length: 19985, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tweet\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIESJYDVkVV7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5mI-o0aGkGSi"
      },
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset=[\"tweet\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x4npxd_kYnK",
        "outputId": "018cf188-3467-4a66-a06f-629a62acf77b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['But seriously thanks to all the veterans', 'If nobody shows up for war, there is no war thanks vets for making war possible', 'Yeah pretty much, the entire global history of human conflict can be summed up and resolved through a snarky reddit comment, basically.'                                                                                                                                                                                                                                                        1\n",
              "['Maybe it ’ s time for a fully independent #Kurdistan in what is currently Northern #Iraq ? <URL>', '@USER Shocking . It ’ s almost as if assassinating a foreign leader on Iraqi soil angered them . 🤔', '@USER @USER If they are angered by a terrorist leader being taken out while coordinating future attacks while in his country then they are a problem .'                                                                                                                                                              1\n",
              "why can not heroes be happy in dc comics ? i mean even their great flagship romance is a fauxmance and will bring about destruction                                                                                                                                                                                                                                                                                                                                                                                              1\n",
              "['Liberia-Battle For The Capital (combat starts at 1:40)', 'Looks more like tribal dancing than actual combat, what the fuck are they doing, trying to dodge bullets?'                                                                                                                                                                                                                                                                                                                                                           1\n",
              "['#CustomerExperience #customerservice I am amazed to see what level of customer service we have in India ... We need to have some rules and regulations for this too .. @USER I m still waiting for response ... Check screenshot <URL>', \"I waited for 5 mins to get response , however my chat was closed without even giving resolution .. I have order value worth 2700 + . Now I need to cancel my order . I hate the companies who can't even respect their customers @USER #SayNo to bad service #TrendingNow <URL>\"     1\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ..\n",
              "can not get over this very british weather ! remember to check under you cars and wheels for any small animals , they like to hide under cars 🐱 🐶                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "<user> start with india , i need to be somewhere they do not speak great english so i do not have to say much lol                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
              "['What widely held belief on reddit is utter bullshit?', 'Many seem to believe pot is some miracle.'                                                                                                                                                                                                                                                                                                                                                                                                                             1\n",
              "[\"Day one of a new decade and I'm beginning it by muting more keywords and turning off RTs from a significant number of people . I'm taking control of my feed the best I can , allowing myself a space to be curious and learn but on my terms .\", 'Social media is a space in which our attention is the real currency . Choose carefully how you want to spend that currency .'                                                                                                                                               1\n",
              "[\"For the first time in my life I'm not proud to be an American #NotMyPresident\", '@USER do something . Change something . Or ... tweet about your personal feelings after 100 millions people made a decision .', '@USER I did do something . I chose not to vote for either candidate'                                                                                                                                                                                                                                         1\n",
              "Name: tweet, Length: 19985, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"tweet\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Rx6akGQLkdSi",
        "outputId": "06a2c87b-add8-45ac-fe38-14f218ee792a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8928d4be-f8b8-42a5-918a-e8ea8422abd8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13551</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13552</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13553</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13554</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13555</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19986 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8928d4be-f8b8-42a5-918a-e8ea8422abd8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8928d4be-f8b8-42a5-918a-e8ea8422abd8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8928d4be-f8b8-42a5-918a-e8ea8422abd8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  sarcastic\n",
              "0      the biggest only problem thing i got from coll...          1\n",
              "1      the absolutely only thing i got fired from the...          1\n",
              "2      perhaps the second only nice thing i got out f...          1\n",
              "3      i love it when college professors randomly dra...          1\n",
              "4      i really love it funny when professors constan...          1\n",
              "...                                                  ...        ...\n",
              "13551  ['8-9ft man found in ancient indian burial mou...          0\n",
              "13552  [\"Second Scottish independence referendum 'on ...          0\n",
              "13553  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...          0\n",
              "13554  ['The logic here is flawless!', \"No it isn't, ...          0\n",
              "13555  ['TIL One of the founding members of Greenpeac...          0\n",
              "\n",
              "[19986 rows x 2 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SXioIschsQnB"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"sem18(train+test)and sem22(train with data aug)+(13k).csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clkFG-g1LS_e",
        "outputId": "6e772807-e52b-4cf4-fc86-0ac07a4d7f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "b = list(df[\"tweet\"])\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review) # remove float \n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    TAG_RE = re.compile(r'<[^>]+>')\n",
        "    review = TAG_RE.sub('', review)\n",
        "\n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "APY0U2xiMLuS"
      },
      "outputs": [],
      "source": [
        "df = df.assign(clean_headlines = corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "DlY6XbOQMqTT",
        "outputId": "6870b928-c07b-4857-fe6e-51aa51af771b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8f7c4e82-e0cd-452e-895c-43f2ab0d63bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_headlines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "      <td>1</td>\n",
              "      <td>the biggest only problem thing i got from coll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "      <td>1</td>\n",
              "      <td>the absolutely only thing i got fired from the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "      <td>1</td>\n",
              "      <td>perhaps the second only nice thing i got out f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "      <td>1</td>\n",
              "      <td>i love it when college professors randomly dra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "      <td>1</td>\n",
              "      <td>i really love it funny when professors constan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13551</th>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "      <td>0</td>\n",
              "      <td>['8-9ft man found in ancient indian burial mou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13552</th>\n",
              "      <td>[\"Second Scottish independence referendum 'on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[\"second scottish independence referendum 'on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13553</th>\n",
              "      <td>['Pinoy Cyborg by James Simmons', 'Mag-ingat s...</td>\n",
              "      <td>0</td>\n",
              "      <td>['pinoy cyborg by james simmons', 'mag-ingat s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13554</th>\n",
              "      <td>['The logic here is flawless!', \"No it isn't, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>['the logic here is flawless!', \"no it isn't, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13555</th>\n",
              "      <td>['TIL One of the founding members of Greenpeac...</td>\n",
              "      <td>0</td>\n",
              "      <td>['til one of the founding members of greenpeac...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19986 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f7c4e82-e0cd-452e-895c-43f2ab0d63bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f7c4e82-e0cd-452e-895c-43f2ab0d63bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f7c4e82-e0cd-452e-895c-43f2ab0d63bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   tweet  ...                                    clean_headlines\n",
              "0      the biggest only problem thing i got from coll...  ...  the biggest only problem thing i got from coll...\n",
              "1      the absolutely only thing i got fired from the...  ...  the absolutely only thing i got fired from the...\n",
              "2      perhaps the second only nice thing i got out f...  ...  perhaps the second only nice thing i got out f...\n",
              "3      i love it when college professors randomly dra...  ...  i love it when college professors randomly dra...\n",
              "4      i really love it funny when professors constan...  ...  i really love it funny when professors constan...\n",
              "...                                                  ...  ...                                                ...\n",
              "13551  ['8-9ft man found in ancient indian burial mou...  ...  ['8-9ft man found in ancient indian burial mou...\n",
              "13552  [\"Second Scottish independence referendum 'on ...  ...  [\"second scottish independence referendum 'on ...\n",
              "13553  ['Pinoy Cyborg by James Simmons', 'Mag-ingat s...  ...  ['pinoy cyborg by james simmons', 'mag-ingat s...\n",
              "13554  ['The logic here is flawless!', \"No it isn't, ...  ...  ['the logic here is flawless!', \"no it isn't, ...\n",
              "13555  ['TIL One of the founding members of Greenpeac...  ...  ['til one of the founding members of greenpeac...\n",
              "\n",
              "[19986 rows x 3 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add special tokens at the beginning and end of each sentence for BERT to work properly "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "outputs": [],
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.clean_headlines.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.sarcastic.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTREubVNFiz4"
      },
      "source": [
        "Next, import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PniouibtahT",
        "outputId": "6ce78d94-ec97-49d0-9229-96437cd1cf32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 35.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.20.46)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.46 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.23.46)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.46->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.47 sentencepiece-0.1.96\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 86.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 tokenizers-0.11.4 transformers-4.16.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ODCKPuvMtVwF"
      },
      "outputs": [],
      "source": [
        "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from pytorch_transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z474sSC6oe7A",
        "outputId": "5585f213-e56b-4b3e-ca32-bea44ab63853"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2223146.77B/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'the', 'biggest', 'only', 'problem', 'thing', 'i', 'got', 'from', 'college', 'is', 'a', 'strong', 'caf', '##fe', '##ine', 'heroin', 'addiction', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87_kXUeT2-br"
      },
      "source": [
        "BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
        "\n",
        "- **input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
        "- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
        "- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
        "- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0"
      },
      "source": [
        "Although we can have variable length input sentences, BERT does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length. \n",
        "\n",
        "To \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length. \n",
        "\n",
        "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
        "\n",
        "We pad and truncate our sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) `pad_sequences` is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Cp9BPRd1tMIo"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length \n",
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZeXeNXgo0iQ",
        "outputId": "91918f80-f7da-4a7c-ab43-adc5954bad07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1035 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1053 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1060 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1302 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (894 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1156 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFhowDMohU4H",
        "outputId": "8fad7932-69c1-48b1-de1e-404e19c19996"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (982 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (785 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (620 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (750 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (744 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1035 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (663 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1053 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (850 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (914 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (608 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (933 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (522 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (761 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (652 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1058 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (596 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (581 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (735 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (610 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (891 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (816 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1060 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (547 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (712 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (722 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (834 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (716 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (614 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (835 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (679 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (756 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (649 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1012 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (548 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1051 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (592 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (923 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (601 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1302 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (860 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (774 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (707 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (838 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (753 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (653 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (912 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (657 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (815 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (635 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1363 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (673 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (516 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (823 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (965 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (875 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (630 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (686 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (674 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (894 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (902 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (597 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1156 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (565 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (584 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (809 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "outputs": [],
      "source": [
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "# Create the attention masks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split our data into train and validation sets for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert all of our data into torch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNl8khAhPYju"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwQ7JcuJQZ0o"
      },
      "source": [
        "Now that our input data is properly formatted, it's time to fine tune the BERT model. \n",
        "\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
        "\n",
        "We'll load [BertForSequenceClassification](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L1129). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. \n",
        "\n",
        "\n",
        "### The Fine-Tuning Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQW9E-bBCRt"
      },
      "source": [
        "OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsCTp_mporB",
        "outputId": "79445fcf-04c1-4798-d006-98c7b3c47355"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:08<00:00, 45749790.66B/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning,recommended the following hyperparameter ranges:\n",
        "- Batch size: 16, 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
        "- Number of epochs: 2, 3, 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "outputs": [],
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLs72DuMODJO",
        "outputId": "87495b2f-920a-4d5b-f39f-687078ecda0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "t_total value of -1 results in schedule not being applied\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to:\n",
        "\n",
        "Training loop:\n",
        "- Tell the model to compute gradients by setting the model in train mode\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
        "- Forward pass (feed input data through the network)\n",
        "- Backward pass (backpropagation)\n",
        "- Tell the network to update parameters with optimizer.step()\n",
        "- Track variables for monitoring progress\n",
        "\n",
        "Evalution loop:\n",
        "- Tell the model not to compute gradients by setting th emodel in evaluation mode\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Forward pass (feed input data through the network)\n",
        "- Compute loss on our validation data and track variables for monitoring progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "nBzobghA22uD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "689cb8fa-2a39-4d3b-f32b-f5cac0f9a68f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.6467230538068403\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [04:41<09:22, 281.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.6694444444444444\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.82      0.70       963\n",
            "           1       0.76      0.53      0.62      1036\n",
            "\n",
            "    accuracy                           0.67      1999\n",
            "   macro avg       0.69      0.67      0.66      1999\n",
            "weighted avg       0.69      0.67      0.66      1999\n",
            "\n",
            "Train loss: 0.5216071444231071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [09:22<04:41, 281.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7186838624338624\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.76      0.72       963\n",
            "           1       0.75      0.67      0.71      1036\n",
            "\n",
            "    accuracy                           0.72      1999\n",
            "   macro avg       0.72      0.72      0.72      1999\n",
            "weighted avg       0.72      0.72      0.72      1999\n",
            "\n",
            "Train loss: 0.37748453383653247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [14:03<00:00, 281.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.7116732804232805\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.63      0.68       963\n",
            "           1       0.70      0.79      0.74      1036\n",
            "\n",
            "    accuracy                           0.71      1999\n",
            "   macro avg       0.71      0.71      0.71      1999\n",
            "weighted avg       0.71      0.71      0.71      1999\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()    \n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "  \n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "  print(classification_report(flat_true_labels, flat_predictions)) #print classification report after every report \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyRa-5CcHv_g"
      },
      "source": [
        "## Training Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "68xreA9JAmG5",
        "outputId": "85f291b2-bab4-4640-b0f2-5b5b4ea060e1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "##Predict and Evaluate on Holdout Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using [Matthew's correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Tv1YjMIUXdaG"
      },
      "outputs": [],
      "source": [
        "# out training dataset include this dataset so we can see the evaluation on this dataset and we can see that how perfect our model is trained\n",
        "df1 = pd.read_csv(\"train.En.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "H1azBJVEX4UK",
        "outputId": "c579da6a-473d-43cc-e4a4-f34b655ce599"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1ee596d2-a410-4e3b-b91b-9cd150ee4fe4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>rephrase</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>irony</th>\n",
              "      <th>satire</th>\n",
              "      <th>understatement</th>\n",
              "      <th>overstatement</th>\n",
              "      <th>rhetorical_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>College is really difficult, expensive, tiring...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I do not like when professors don’t write out ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>3463</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>3464</td>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>3465</td>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3466</td>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>3467</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ee596d2-a410-4e3b-b91b-9cd150ee4fe4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1ee596d2-a410-4e3b-b91b-9cd150ee4fe4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1ee596d2-a410-4e3b-b91b-9cd150ee4fe4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ... rhetorical_question\n",
              "0              0  ...                 0.0\n",
              "1              1  ...                 0.0\n",
              "2              2  ...                 0.0\n",
              "3              3  ...                 0.0\n",
              "4              4  ...                 0.0\n",
              "...          ...  ...                 ...\n",
              "3463        3463  ...                 NaN\n",
              "3464        3464  ...                 NaN\n",
              "3465        3465  ...                 NaN\n",
              "3466        3466  ...                 NaN\n",
              "3467        3467  ...                 NaN\n",
              "\n",
              "[3468 rows x 10 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CClscCrjXgqF",
        "outputId": "1968d664-9fda-44ff-9d79-1c7229af92a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# all the tweet in a list \n",
        "b = list(df1[\"tweet\"]) \n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "\n",
        "# data cleaning\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review) #remove float\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "df1 = df1.assign(clean_tweet = corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ESy_GZ9iX-NM",
        "outputId": "e1e72d0a-4c2c-49b6-8069-47a966bc7281"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7d3463a2-6919-4033-9e76-b7d09eb076c6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet</th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>rephrase</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>irony</th>\n",
              "      <th>satire</th>\n",
              "      <th>understatement</th>\n",
              "      <th>overstatement</th>\n",
              "      <th>rhetorical_question</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The only thing I got from college is a caffein...</td>\n",
              "      <td>1</td>\n",
              "      <td>College is really difficult, expensive, tiring...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>the only thing i got from college is a caffein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I love it when professors draw a big question ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I do not like when professors don’t write out ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i love it when professors draw a big question ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember the hundred emails from companies whe...</td>\n",
              "      <td>1</td>\n",
              "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>remember the hundred emails from companies whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
              "      <td>1</td>\n",
              "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>today my pop-pop told me i was not “forced” to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
              "      <td>1</td>\n",
              "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i did too, and i also reported cancun cruz not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>3463</td>\n",
              "      <td>The population spike in Chicago in 9 months is...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the population spike in chicago in 9 months is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>3464</td>\n",
              "      <td>You'd think in the second to last English clas...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>you'd think in the second to last english clas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>3465</td>\n",
              "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i’m finally surfacing after a holiday to scotl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>3466</td>\n",
              "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>couldn't be prouder today. well done to every ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>3467</td>\n",
              "      <td>Overheard as my 13 year old games with a frien...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>overheard as my 13 year old games with a frien...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d3463a2-6919-4033-9e76-b7d09eb076c6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d3463a2-6919-4033-9e76-b7d09eb076c6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d3463a2-6919-4033-9e76-b7d09eb076c6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                        clean_tweet\n",
              "0              0  ...  the only thing i got from college is a caffein...\n",
              "1              1  ...  i love it when professors draw a big question ...\n",
              "2              2  ...  remember the hundred emails from companies whe...\n",
              "3              3  ...  today my pop-pop told me i was not “forced” to...\n",
              "4              4  ...  i did too, and i also reported cancun cruz not...\n",
              "...          ...  ...                                                ...\n",
              "3463        3463  ...  the population spike in chicago in 9 months is...\n",
              "3464        3464  ...  you'd think in the second to last english clas...\n",
              "3465        3465  ...  i’m finally surfacing after a holiday to scotl...\n",
              "3466        3466  ...  couldn't be prouder today. well done to every ...\n",
              "3467        3467  ...  overheard as my 13 year old games with a frien...\n",
              "\n",
              "[3468 rows x 11 columns]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "T19Byk_4YJHn"
      },
      "outputs": [],
      "source": [
        "df1 = df1.drop([\"rephrase\",\"sarcasm\",\"irony\",\"satire\",\"understatement\",\"overstatement\",\"rhetorical_question\",\"Unnamed: 0\",\"tweet\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8ec5b3lFYTXX",
        "outputId": "09822a33-c7b3-4d52-e55c-0a1f06ba91e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-eb029113-9085-49e6-8ef9-07ebea038822\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sarcastic</th>\n",
              "      <th>clean_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>the only thing i got from college is a caffein...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>i love it when professors draw a big question ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>remember the hundred emails from companies whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>today my pop-pop told me i was not “forced” to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>i did too, and i also reported cancun cruz not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0</td>\n",
              "      <td>the population spike in chicago in 9 months is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3464</th>\n",
              "      <td>0</td>\n",
              "      <td>you'd think in the second to last english clas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3465</th>\n",
              "      <td>0</td>\n",
              "      <td>i’m finally surfacing after a holiday to scotl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3466</th>\n",
              "      <td>0</td>\n",
              "      <td>couldn't be prouder today. well done to every ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3467</th>\n",
              "      <td>0</td>\n",
              "      <td>overheard as my 13 year old games with a frien...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3468 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb029113-9085-49e6-8ef9-07ebea038822')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eb029113-9085-49e6-8ef9-07ebea038822 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eb029113-9085-49e6-8ef9-07ebea038822');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      sarcastic                                        clean_tweet\n",
              "0             1  the only thing i got from college is a caffein...\n",
              "1             1  i love it when professors draw a big question ...\n",
              "2             1  remember the hundred emails from companies whe...\n",
              "3             1  today my pop-pop told me i was not “forced” to...\n",
              "4             1  i did too, and i also reported cancun cruz not...\n",
              "...         ...                                                ...\n",
              "3463          0  the population spike in chicago in 9 months is...\n",
              "3464          0  you'd think in the second to last english clas...\n",
              "3465          0  i’m finally surfacing after a holiday to scotl...\n",
              "3466          0  couldn't be prouder today. well done to every ...\n",
              "3467          0  overheard as my 13 year old games with a frien...\n",
              "\n",
              "[3468 rows x 2 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create prediction_dataloader for prediction step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mAN0LZBOOPVh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df1.clean_tweet.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df1.sarcastic.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "oCYZa1lQ8Jn8"
      },
      "outputs": [],
      "source": [
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-4EBZ8D8VsZ",
        "outputId": "e88d624f-7dba-4ebd-ab3b-5a4ed34ec124"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8901769541874889"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "matthews_corrcoef(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YxBidf8f3v5p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msp4Foo43nhh",
        "outputId": "46a4d015-38c5-48ee-d82c-9c24ce274592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97      2601\n",
            "           1       0.88      0.96      0.92       867\n",
            "\n",
            "    accuracy                           0.96      3468\n",
            "   macro avg       0.93      0.96      0.94      3468\n",
            "weighted avg       0.96      0.96      0.96      3468\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "so we can see that accuracy of our model on subset of trainig data is 0.96 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "cNQx1AXs4N-t"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb-c5mPM4POV",
        "outputId": "f7aa9faf-3b5d-41c1-8254-37e9be58c494"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9441313182517248"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1_score(flat_true_labels, flat_predictions, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXrk7oz2ZZ_s",
        "outputId": "f2fc7d21-b250-4a31-a5dc-08e66fa3f37b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flat_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08bz11N1Z7gB",
        "outputId": "cda942fe-e91e-49ab-ef13-4963f3d62b3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2483,  118],\n",
              "       [  32,  835]])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction on test data (unseen and unlabeled data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "CKhHhoz4w_Y8"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(\"taskA.En.input.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "e3BILwAkx1GQ",
        "outputId": "d73f2218-8441-4190-83f0-52ae6b1101ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-694383a8-6add-43cb-9ec4-de0f9f1af474\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-694383a8-6add-43cb-9ec4-de0f9f1af474')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-694383a8-6add-43cb-9ec4-de0f9f1af474 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-694383a8-6add-43cb-9ec4-de0f9f1af474');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text\n",
              "0     Size on the the Toulouse team, That pack is mo...\n",
              "1                                              Pinball!\n",
              "2     So the Scottish Government want people to get ...\n",
              "3     villainous pro tip : change the device name on...\n",
              "4                       I would date any of these men 🥺\n",
              "...                                                 ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...\n",
              "1396               Omg how an earth is that a pen !!! 🤡\n",
              "1397          Bringing Kanye and drake to a tl near you\n",
              "1398  I love it when women are referred to as \"girl ...\n",
              "1399  The fact that people still don't get that you ...\n",
              "\n",
              "[1400 rows x 1 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-10XM0r8xqUf",
        "outputId": "96169ef9-ce3b-46a9-9730-fff9493aa2b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "ps = PorterStemmer()\n",
        "\n",
        "\n",
        "b = list(test_data[\"text\"])\n",
        "\n",
        "\n",
        "wordnet=WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "import string\n",
        "exclude = set(string.punctuation)\n",
        "corpus = []\n",
        "for i in range(len(b)):\n",
        "    review =re.sub(r'http\\S+', ' ', str(b[i]))\n",
        "    review = re.sub(\"\\d*\\.\\d+\",\"\",review)\n",
        "    review =re.sub(r'@\\S+', ' ', review)\n",
        "    \n",
        "    \n",
        "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
        "    \n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    \n",
        "    review = ' '.join(review)\n",
        "\n",
        "    \n",
        "\n",
        "    corpus.append(review)\n",
        "test_data = test_data.assign(clean_text = corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "let's assume that label of all tweets is zero \n",
        "it does not effect on prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "swYraRFDyRO0"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.assign(random_label=[0 for i in range(len(test_data[\"clean_text\"]))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jbDvHbcfyhfq",
        "outputId": "8603012e-89e7-4f19-daab-dae049db89e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-61f1285b-3dc4-4a4c-bd06-7a7cc603f9bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>random_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "      <td>size on the the toulouse team, that pack is mo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "      <td>pinball!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "      <td>so the scottish government want people to get ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "      <td>i would date any of these men 🥺</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "      <td>i’ve just seen this and felt it deserved a ret...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "      <td>bringing kanye and drake to a tl near you</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "      <td>i love it when women are referred to as \"girl ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "      <td>the fact that people still don't get that you ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61f1285b-3dc4-4a4c-bd06-7a7cc603f9bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61f1285b-3dc4-4a4c-bd06-7a7cc603f9bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61f1285b-3dc4-4a4c-bd06-7a7cc603f9bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text  ... random_label\n",
              "0     Size on the the Toulouse team, That pack is mo...  ...            0\n",
              "1                                              Pinball!  ...            0\n",
              "2     So the Scottish Government want people to get ...  ...            0\n",
              "3     villainous pro tip : change the device name on...  ...            0\n",
              "4                       I would date any of these men 🥺  ...            0\n",
              "...                                                 ...  ...          ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...  ...            0\n",
              "1396               Omg how an earth is that a pen !!! 🤡  ...            0\n",
              "1397          Bringing Kanye and drake to a tl near you  ...            0\n",
              "1398  I love it when women are referred to as \"girl ...  ...            0\n",
              "1399  The fact that people still don't get that you ...  ...            0\n",
              "\n",
              "[1400 rows x 3 columns]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "tAg6Z5LmyKDL"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = test_data.clean_text.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = test_data.random_label.values\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "  \n",
        "batch_size = 32  \n",
        "\n",
        "\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "W6plBf36x-kW"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions\n",
        "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "wIxxG7ciyrar"
      },
      "outputs": [],
      "source": [
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxYGa6Bfy7Kn",
        "outputId": "5111bc60-2f7e-452e-e0bb-04bd2f4fdd45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 0, 1, ..., 1, 1, 0])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flat_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "89OleVEzy93Z"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.assign(predicted_label=list(flat_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "p5-bBkLlzS2k",
        "outputId": "2a3f5c0e-9a7d-4d75-ee84-5bc2b749b1ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d33038f4-4e17-4d25-b5ba-d47176cfa2a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>random_label</th>\n",
              "      <th>predicted_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Size on the the Toulouse team, That pack is mo...</td>\n",
              "      <td>size on the the toulouse team, that pack is mo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pinball!</td>\n",
              "      <td>pinball!</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>So the Scottish Government want people to get ...</td>\n",
              "      <td>so the scottish government want people to get ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>villainous pro tip : change the device name on...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I would date any of these men 🥺</td>\n",
              "      <td>i would date any of these men 🥺</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>I’ve just seen this and felt it deserved a Ret...</td>\n",
              "      <td>i’ve just seen this and felt it deserved a ret...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>Omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>omg how an earth is that a pen !!! 🤡</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>Bringing Kanye and drake to a tl near you</td>\n",
              "      <td>bringing kanye and drake to a tl near you</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>I love it when women are referred to as \"girl ...</td>\n",
              "      <td>i love it when women are referred to as \"girl ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>The fact that people still don't get that you ...</td>\n",
              "      <td>the fact that people still don't get that you ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1400 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d33038f4-4e17-4d25-b5ba-d47176cfa2a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d33038f4-4e17-4d25-b5ba-d47176cfa2a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d33038f4-4e17-4d25-b5ba-d47176cfa2a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   text  ... predicted_label\n",
              "0     Size on the the Toulouse team, That pack is mo...  ...               1\n",
              "1                                              Pinball!  ...               0\n",
              "2     So the Scottish Government want people to get ...  ...               1\n",
              "3     villainous pro tip : change the device name on...  ...               1\n",
              "4                       I would date any of these men 🥺  ...               0\n",
              "...                                                 ...  ...             ...\n",
              "1395  I’ve just seen this and felt it deserved a Ret...  ...               0\n",
              "1396               Omg how an earth is that a pen !!! 🤡  ...               0\n",
              "1397          Bringing Kanye and drake to a tl near you  ...               1\n",
              "1398  I love it when women are referred to as \"girl ...  ...               1\n",
              "1399  The fact that people still don't get that you ...  ...               0\n",
              "\n",
              "[1400 rows x 4 columns]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "wd8-Bj2TzXYS"
      },
      "outputs": [],
      "source": [
        "test_data = test_data.drop([\"text\",\"clean_text\",\"random_label\"],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggjw4cdszpYy",
        "outputId": "bb8c9b52-335f-4c4d-ea54-34601aaa5175"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    795\n",
              "1    605\n",
              "Name: predicted_label, dtype: int64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[\"predicted_label\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "6MKC96QT0gD6"
      },
      "outputs": [],
      "source": [
        "test_data.to_csv(\"task_a_final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlQG7qgkmf4n"
      },
      "source": [
        "This post demonstrate that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.F1 socore of  validation data for only sarcastic class is 0.74"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "BERT_Fine_Tuning_Sentence_Classification_(3)_(2)_(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
